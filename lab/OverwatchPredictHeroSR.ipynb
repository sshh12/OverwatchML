{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "from OverwatchProcessData import get_vector_herostats\n",
    "from OverwatchProcessData import get_competitive_rank, hero_stats\n",
    "from OverwatchGatherData import Player, find_usernames\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "np.random.seed(3)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 116)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Data\n",
    "\n",
    "specific_stats = {}\n",
    "\n",
    "for stat in hero_stats:\n",
    "    \n",
    "    hero, _, _ = stat.split(\" \")\n",
    "    \n",
    "    if hero in specific_stats:\n",
    "        \n",
    "        specific_stats[hero].append(stat)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        specific_stats[hero] = [stat]\n",
    "\n",
    "def generate_players(limit=1e10):\n",
    "    \n",
    "    for filename in os.listdir(os.path.join('..', 'profiles')):\n",
    "        \n",
    "        if limit < 0: return\n",
    "        \n",
    "        try: # If it can't read the player then just ignore file\n",
    "            \n",
    "            player = Player.from_file(os.path.join('..', 'profiles', filename))\n",
    "        \n",
    "            if 'error' not in player.json and get_competitive_rank(player, 'us'):\n",
    "\n",
    "                yield player\n",
    "                \n",
    "                limit -= 1\n",
    "                \n",
    "            else: # Throw Away\n",
    "                \n",
    "                print('Deleting Profile...', filename)\n",
    "                \n",
    "                os.remove(os.path.join('..', 'profiles', filename))\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "def load_data():\n",
    "    \n",
    "    raw_data = {}\n",
    "\n",
    "    for player in generate_players():\n",
    "\n",
    "        rank = get_competitive_rank(player, 'us')\n",
    "\n",
    "        if rank:\n",
    "            \n",
    "            comp_stats = player.json['us']['heroes']['stats']['competitive']\n",
    "                \n",
    "            for hero in comp_stats:\n",
    "                \n",
    "                try:\n",
    "                    time_played = comp_stats[hero]['general_stats']['time_played']\n",
    "                except:\n",
    "                    time_played = -1\n",
    "\n",
    "                if time_played >= .4: # Min Time Played\n",
    "                        \n",
    "                    if hero not in raw_data: raw_data[hero] = [], []\n",
    "\n",
    "                    raw_data[hero][0].append(get_vector_herostats(player, 'us', stat_keys=specific_stats[hero]))\n",
    "                    raw_data[hero][1].append(rank)\n",
    "            \n",
    "    for hero in raw_data:\n",
    "        \n",
    "        unscaled_X, unscaled_y = raw_data[hero]\n",
    "\n",
    "        unscaled_X = np.array(unscaled_X, dtype=np.float64)\n",
    "        unscaled_y = np.array(unscaled_y, dtype=np.float64)\n",
    "    \n",
    "        print(hero.title(), unscaled_X.shape, unscaled_y.shape)\n",
    "        \n",
    "        raw_data[hero] = unscaled_X, unscaled_y\n",
    "    \n",
    "    return raw_data\n",
    "\n",
    "len(specific_stats), len(specific_stats['mercy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scale\n",
    "\n",
    "def scale_data(hero_data):\n",
    "    \n",
    "    unscaled_X, unscaled_y = hero_data\n",
    "    \n",
    "    scaler_X = StandardScaler()\n",
    "\n",
    "    X = scaler_X.fit_transform(unscaled_X)\n",
    "    y = unscaled_y / 5000\n",
    "    \n",
    "    return X, y, scaler_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Metric\n",
    "\n",
    "def acc_metric(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Accuracy\n",
    "    \"\"\"\n",
    "    diff = K.abs(y_pred - y_true) * 5000\n",
    "    \n",
    "    return K.mean(diff, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "def get_model(hero):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(40, input_dim=len(specific_stats[hero]), kernel_initializer='normal', activation='selu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Dense(40, kernel_initializer='normal', activation='selu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Dense(40, kernel_initializer='normal', activation='selu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Dense(40, kernel_initializer='normal', activation='selu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Dense(40, kernel_initializer='normal', activation='selu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Dense(40, kernel_initializer='normal', activation='selu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=[acc_metric])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train wrapper\n",
    "\n",
    "def train_model(model, *args, **kwargs):\n",
    "\n",
    "    history = model.fit(*args, **kwargs, shuffle=True, validation_split=.10, verbose=0)\n",
    "    \n",
    "    return history\n",
    "\n",
    "def get_hero_model(hero, hero_data=None, from_file=False):\n",
    "    \n",
    "    if from_file:\n",
    "        \n",
    "        model = load_model(os.path.join('..', 'models', '{}-sr.h5'.format(hero)))\n",
    "        \n",
    "        scaler_X = joblib.load(os.path.join('..', 'models', '{}-sr.pkl'.format(hero)))\n",
    "        \n",
    "        return None, model, scaler_X\n",
    "    \n",
    "    X, y, scaler_X = hero_data\n",
    "\n",
    "    model = get_model(hero)\n",
    "    \n",
    "    ## Callbacks ##\n",
    "    reduce_LR = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=20, min_lr=1e-7, verbose=0)\n",
    "    e_stopping = EarlyStopping(patience=35)\n",
    "    checkpoint = ModelCheckpoint(os.path.join('..', 'models', '{}-sr.h5'.format(hero)), \n",
    "                                 monitor='val_acc_metric', \n",
    "                                 verbose=0,\n",
    "                                 save_best_only=True)\n",
    "    ###############\n",
    "    \n",
    "    joblib.dump(scaler_X, os.path.join('..', 'models', '{}-sr.pkl'.format(hero)))\n",
    "\n",
    "    history = train_model(model, X, y, epochs=1500, batch_size=128, callbacks=[reduce_LR, e_stopping, checkpoint])\n",
    "    \n",
    "    return history, model, scaler_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "\n",
    "def predict_sr(model, player, scaler_for_X, hero):\n",
    "    \n",
    "    stats_vector = np.array([get_vector_herostats(player, 'us', stat_keys=specific_stats[hero])])\n",
    "    \n",
    "    X = scaler_for_X.transform(stats_vector)\n",
    "\n",
    "    y_matrix = model.predict(X)\n",
    "    \n",
    "    sr = np.squeeze(y_matrix) * 5000\n",
    "    \n",
    "    return int(sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dva (17278, 109) (17278,)\n",
      "Junkrat (12484, 97) (12484,)\n",
      "Lucio (15499, 114) (15499,)\n",
      "Torbjorn (4553, 108) (4553,)\n",
      "Mercy (19950, 116) (19950,)\n",
      "Mei (4430, 108) (4430,)\n",
      "Pharah (13188, 94) (13188,)\n",
      "Roadhog (15629, 111) (15629,)\n",
      "Mccree (12680, 96) (12680,)\n",
      "Zenyatta (15419, 103) (15419,)\n",
      "Reaper (10868, 99) (10868,)\n",
      "Zarya (15992, 114) (15992,)\n",
      "Soldier76 (18078, 114) (18078,)\n",
      "Winston (16217, 110) (16217,)\n",
      "Genji (9828, 96) (9828,)\n",
      "Orisa (10526, 93) (10526,)\n",
      "Ana (15465, 115) (15465,)\n",
      "Bastion (5369, 107) (5369,)\n",
      "Symmetra (4777, 97) (4777,)\n",
      "Tracer (10723, 102) (10723,)\n",
      "Moira (9058, 94) (9058,)\n",
      "Doomfist (1923, 87) (1923,)\n",
      "Sombra (2443, 91) (2443,)\n",
      "Hanzo (6340, 103) (6340,)\n",
      "Reinhardt (18099, 98) (18099,)\n",
      "Widowmaker (7084, 101) (7084,)\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    all_data = {}\n",
    "    \n",
    "    raw_data = load_data()\n",
    "    \n",
    "    for hero in raw_data:\n",
    "        \n",
    "        all_data[hero] = scale_data(raw_data[hero])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    history_db = {}\n",
    "\n",
    "    f, (loss_plot, acc_plot) = plt.subplots(2, 1, sharex=True)\n",
    "\n",
    "    for hero in specific_stats:\n",
    "\n",
    "        print('Training ' + hero.title())\n",
    "\n",
    "        history, model, _ = get_hero_model(hero, hero_data=all_data[hero])\n",
    "\n",
    "        loss, acc = np.log(history.history['val_loss']), history.history['val_acc_metric']\n",
    "\n",
    "        loss_plot.plot(loss)\n",
    "        acc_plot.plot(acc)\n",
    "\n",
    "        history_db[hero] = [loss, acc]\n",
    "\n",
    "        print(np.mean(acc[-10]))\n",
    "\n",
    "    loss_plot.set_title('Log Loss')\n",
    "    acc_plot.set_title('Accuracy')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load models from disk\n",
    "\n",
    "import keras.metrics\n",
    "keras.metrics.acc_metric = acc_metric # Weird Patch\n",
    "\n",
    "models = {}\n",
    "\n",
    "for hero in specific_stats:\n",
    "    \n",
    "    models[hero] = get_hero_model(hero, from_file=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict using all viable models\n",
    "\n",
    "def predict_all(player):\n",
    "    \n",
    "    sr_predictions = []\n",
    "    time_played = []\n",
    "\n",
    "    for hero in specific_stats:\n",
    "        \n",
    "        player_hero_stats = player.json['us']['heroes']['stats']['competitive']\n",
    "\n",
    "        if hero in player_hero_stats and player_hero_stats[hero]['general_stats']['time_played'] >= .4:\n",
    "\n",
    "            _, model, scaler = models[hero]\n",
    "                \n",
    "            sr_predictions.append(predict_sr(model, player, scaler, hero))\n",
    "            time_played.append(player_hero_stats[hero]['general_stats']['time_played'])\n",
    "                \n",
    "    return int(np.average(sr_predictions, weights=time_played))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "with open('test_names.txt', 'r') as test:\n",
    "\n",
    "    for battletag in find_usernames(test.read()):\n",
    "        \n",
    "        player = Player.from_web_battletag(battletag)\n",
    "        \n",
    "        actual = get_competitive_rank(player, 'us')\n",
    "        p = predict_all(player)\n",
    "        \n",
    "        print(\"{} is {}, predicted {}\".format(battletag, actual, p))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-cpu]",
   "language": "python",
   "name": "conda-env-tf-cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
